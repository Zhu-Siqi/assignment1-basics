{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2bd2dc3",
   "metadata": {},
   "source": [
    "#  Problem (unicode1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b1e48",
   "metadata": {},
   "source": [
    " (a) What Unicode character does chr(0) return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d14fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33949740",
   "metadata": {},
   "source": [
    " (b) How does this character’s string representation (`__repr__()`) differ from its printed representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326712ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\u0000world\n"
     ]
    }
   ],
   "source": [
    "print('hello' + chr(0) + 'world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c5f7777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'hello\\\\x00world'\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr('hello' + chr(0) + 'world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd82df",
   "metadata": {},
   "source": [
    "打印以可读性为目标展示最终输出，会有不可见字符；`repr()`使用无歧义的转义字符以进行调试和理解等！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fc2e2",
   "metadata": {},
   "source": [
    "(c) What happens when this character occurs in text? It may be helpful to play around with the following in your Python interpreter and see if it matches your expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81def2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n",
      "hello\u0000world\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))\n",
    "print('hello' + chr(0) + 'world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0a4af7",
   "metadata": {},
   "source": [
    "#  Problem (unicode2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d5f7ab",
   "metadata": {},
   "source": [
    "**Unicode Encoding**:  it’s impractical to train tokenizers directly on Unicode codepoints, since the vocabulary would be prohibitively large (around 150K items) and sparse (since many characters are quite rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d1ec5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
      "<class 'bytes'>\n",
      "[104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33]\n",
      "List length: 23\n",
      "String length: 13\n",
      "hello! こんにちは!\n"
     ]
    }
   ],
   "source": [
    "# Example of UTF-8 encode\n",
    "test_string = 'hello! こんにちは!'\n",
    "utf8_encode = test_string.encode('utf-8')\n",
    "print(utf8_encode)\n",
    "print(type(utf8_encode)) # 'bytes': 8字节2进制数据 不可变\n",
    "utf8_encode_list = list(utf8_encode)\n",
    "print(utf8_encode_list)\n",
    "\n",
    "# UTF-8为可变长编码方法\n",
    "print(f'List length: {len(utf8_encode_list)}')\n",
    "print(f'String length: {len(test_string)}')\n",
    "\n",
    "print(utf8_encode.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f78dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xfeh\\x00e\\x00l\\x00l\\x00o\\x00!\\x00 \\x00S0\\x930k0a0o0!\\x00'\n",
      "<class 'bytes'>\n",
      "[255, 254, 104, 0, 101, 0, 108, 0, 108, 0, 111, 0, 33, 0, 32, 0, 83, 48, 147, 48, 107, 48, 97, 48, 111, 48, 33, 0]\n",
      "List length: 28\n",
      "String length: 13\n",
      "hello! こんにちは!\n"
     ]
    }
   ],
   "source": [
    "# Example of UTF-16 encode\n",
    "test_string = 'hello! こんにちは!'\n",
    "utf16_encode = test_string.encode('utf-16')\n",
    "print(utf16_encode)\n",
    "print(type(utf16_encode))\n",
    "utf16_encode_list = list(utf16_encode)\n",
    "print(utf16_encode_list)\n",
    "\n",
    "# UTF-16为可变长编码方法\n",
    "print(f'List length: {len(utf16_encode_list)}')\n",
    "print(f'String length: {len(test_string)}')\n",
    "\n",
    "print(utf16_encode.decode('utf-16'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c724343",
   "metadata": {},
   "source": [
    "(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings.\n",
    "\n",
    "1. UTF-8 空间最高效\n",
    "2. UTF-8 最为通用\n",
    "3. UTF-16/32 需要额外的BOM标记符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037bf87",
   "metadata": {},
   "source": [
    "(b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.\n",
    "\n",
    "(c) Give a two byte sequence that does not decode to any Unicode character(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d23cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "an incorrect example: 草\n"
     ]
    }
   ],
   "source": [
    "# 只对1byte字符有效\n",
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "print(decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\")))\n",
    "try:\n",
    "    print(decode_utf8_bytes_to_str_wrong(\"草\".encode(\"utf-8\")))\n",
    "except:\n",
    "    print(f'an incorrect example: 草')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b99c0dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-tokenizer for fast merge compute\n",
    "import regex as re\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
