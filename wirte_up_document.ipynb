{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2bd2dc3",
   "metadata": {},
   "source": [
    "#  Problem (unicode1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b1e48",
   "metadata": {},
   "source": [
    " (a) What Unicode character does chr(0) return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d14fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33949740",
   "metadata": {},
   "source": [
    " (b) How does this character’s string representation (`__repr__()`) differ from its printed representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326712ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello' + chr(0) + 'world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f7777",
   "metadata": {},
   "outputs": [],
   "source": [
    "repr('hello' + chr(0) + 'world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd82df",
   "metadata": {},
   "source": [
    "打印以可读性为目标展示最终输出，会有不可见字符；`repr()`使用无歧义的转义字符以进行调试和理解等！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fc2e2",
   "metadata": {},
   "source": [
    "(c) What happens when this character occurs in text? It may be helpful to play around with the following in your Python interpreter and see if it matches your expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81def2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chr(0))\n",
    "print('hello' + chr(0) + 'world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0a4af7",
   "metadata": {},
   "source": [
    "#  Problem (unicode2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d5f7ab",
   "metadata": {},
   "source": [
    "**Unicode Encoding**:  it’s impractical to train tokenizers directly on Unicode codepoints, since the vocabulary would be prohibitively large (around 150K items) and sparse (since many characters are quite rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ec5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of UTF-8 encode\n",
    "test_string = 'hello! こんにちは!'\n",
    "utf8_encode = test_string.encode('utf-8')\n",
    "print(utf8_encode)\n",
    "print(type(utf8_encode)) # 'bytes': 8字节2进制数据 不可变\n",
    "utf8_encode_list = list(utf8_encode)\n",
    "print(utf8_encode_list)\n",
    "\n",
    "# UTF-8为可变长编码方法\n",
    "print(f'List length: {len(utf8_encode_list)}')\n",
    "print(f'String length: {len(test_string)}')\n",
    "\n",
    "print(utf8_encode.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f78dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of UTF-16 encode\n",
    "test_string = 'hello! こんにちは!'\n",
    "utf16_encode = test_string.encode('utf-16')\n",
    "print(utf16_encode)\n",
    "print(type(utf16_encode))\n",
    "utf16_encode_list = list(utf16_encode)\n",
    "print(utf16_encode_list)\n",
    "\n",
    "# UTF-16为可变长编码方法\n",
    "print(f'List length: {len(utf16_encode_list)}')\n",
    "print(f'String length: {len(test_string)}')\n",
    "\n",
    "print(utf16_encode.decode('utf-16'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c724343",
   "metadata": {},
   "source": [
    "(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings.\n",
    "\n",
    "1. UTF-8 空间最高效\n",
    "2. UTF-8 最为通用\n",
    "3. UTF-16/32 需要额外的BOM标记符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037bf87",
   "metadata": {},
   "source": [
    "(b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.\n",
    "\n",
    "(c) Give a two byte sequence that does not decode to any Unicode character(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只对1byte字符有效\n",
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "print(decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\")))\n",
    "try:\n",
    "    print(decode_utf8_bytes_to_str_wrong(\"草\".encode(\"utf-8\")))\n",
    "except:\n",
    "    print(f'an incorrect example: 草')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-tokenizer for fast merge compute\n",
    "import regex as re\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1897cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just test my implementation (fast enough)\n",
    "from cs336_basics.Tokenizer.tokenizer import run_train_bpe\n",
    "\n",
    "vocab, merges = run_train_bpe(\n",
    "    'data/TinyStoriesV2-GPT4-train.txt',\n",
    "    32000,\n",
    "    ['<|endoftext|>'],\n",
    "    **{'n_processes': 8}\n",
    ")\n",
    "\n",
    "print(vocab[20000])\n",
    "print(merges[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d48de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and save (1) vocabulary (2) merges (3) encoding over openweb\n",
    "import base64\n",
    "import json\n",
    "from cs336_basics.Tokenizer.tokenizer import run_train_bpe, Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "# train vocab and merges\n",
    "vocab, merges = run_train_bpe(\n",
    "    'data/TinyStoriesV2-GPT4-train.txt',\n",
    "    10000,\n",
    "    ['<|endoftext|>'],\n",
    "    **{'n_processes': 8}\n",
    ")\n",
    "\n",
    "# save vocab and merges\n",
    "# without loss encode\n",
    "vocab = {idx: base64.b64encode(token).decode('ascii')  for idx, token in vocab.items()}\n",
    "with open('data/TinyStoriesV2-GPT4-vocab.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(vocab, f, ensure_ascii=False, indent = 2)\n",
    "\n",
    "with open('data/TinyStoriesV2-GPT4-merges.txt', 'w', encoding = 'utf-8') as f:\n",
    "    for pair in merges:\n",
    "        first = base64.b64encode(pair[0]).decode('ascii')\n",
    "        second = base64.b64encode(pair[1]).decode('ascii')\n",
    "        f.write(f'{first} {second}\\n')\n",
    "\n",
    "\n",
    "# load vocab and merges\n",
    "with open('data/TinyStoriesV2-GPT4-vocab.json', 'r', encoding = 'utf-8') as f:\n",
    "    vocab = json.load(f)\n",
    "vocab = {int(idx): base64.b64decode(encode_b) for idx, encode_b in vocab.items()}\n",
    "\n",
    "merges = []\n",
    "with open('data/TinyStoriesV2-GPT4-merges.txt', 'r', encoding = 'utf-8') as f:\n",
    "    for row in f:\n",
    "        if not row:\n",
    "            continue\n",
    "        parts = row.split()\n",
    "        if len(parts) == 2:\n",
    "            first = base64.b64decode(parts[0])\n",
    "            second = base64.b64decode(parts[1])\n",
    "            merges.append((first, second))\n",
    "\n",
    "tkn = Tokenizer(\n",
    "    vocab, merges, ['<|endoftext|>']\n",
    ")\n",
    "\n",
    "# encode the owt train/valid within block\n",
    "for stage in ['train', 'valid']:\n",
    "    encodes = []\n",
    "    with open(f'data/TinyStoriesV2-GPT4-{stage}.txt', 'r', encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            if not line:\n",
    "                continue\n",
    "            else:\n",
    "                encodes.extend(tkn.encode(line))\n",
    "    encodes = np.array(encodes, dtype = np.uint16)\n",
    "    np.save(f'data/TinyStoriesV2-GPT4-{stage}.npy', encodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796fc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and save (1) vocabulary (2) merges (3) encoding over openweb\n",
    "import base64\n",
    "import json\n",
    "from cs336_basics.Tokenizer.tokenizer import run_train_bpe, Tokenizer\n",
    "from multiprocessing import Pool\n",
    "import numpy as np\n",
    "\n",
    "# # train vocab and merges\n",
    "# vocab, merges = run_train_bpe(\n",
    "#     'data/owt_train.txt',\n",
    "#     32000,\n",
    "#     ['<|endoftext|>'],\n",
    "#     **{'n_processes': 8}\n",
    "# )\n",
    "\n",
    "# # save vocab and merges\n",
    "# # without loss encode\n",
    "# vocab = {idx: base64.b64encode(token).decode('ascii')  for idx, token in vocab.items()}\n",
    "# with open('data/owt-vocab.json', 'w', encoding = 'utf-8') as f:\n",
    "#     json.dump(vocab, f, ensure_ascii=False, indent = 2)\n",
    "\n",
    "# with open('data/owt-merges.txt', 'w', encoding = 'utf-8') as f:\n",
    "#     for pair in merges:\n",
    "#         first = base64.b64encode(pair[0]).decode('ascii')\n",
    "#         second = base64.b64encode(pair[1]).decode('ascii')\n",
    "#         f.write(f'{first} {second}\\n')\n",
    "\n",
    "\n",
    "# load vocab and merges\n",
    "with open('data/owt_vocab.json', 'r', encoding = 'utf-8') as f:\n",
    "    vocab = json.load(f)\n",
    "vocab = {int(idx): base64.b64decode(encode_b) for idx, encode_b in vocab.items()}\n",
    "\n",
    "merges = []\n",
    "with open('data/owt_merges.txt', 'r', encoding = 'utf-8') as f:\n",
    "    for row in f:\n",
    "        if not row:\n",
    "            continue\n",
    "        parts = row.split()\n",
    "        if len(parts) == 2:\n",
    "            first = base64.b64decode(parts[0])\n",
    "            second = base64.b64decode(parts[1])\n",
    "            merges.append((first, second))\n",
    "\n",
    "tkn = Tokenizer(\n",
    "    vocab, merges, ['<|endoftext|>']\n",
    ")\n",
    "\n",
    "def encode_chunk(chunk_lines):\n",
    "    \"\"\"\n",
    "    对一部分文本行（一个数据块）进行编码。\n",
    "    这个函数将在每个子进程中运行。\n",
    "    \"\"\"\n",
    "    # 重新初始化分词器或确保分词器可以被子进程安全使用\n",
    "    # 在这个例子中，tkn 对象是只读的，可以被子进程安全地继承\n",
    "    encoded_tokens = []\n",
    "    for line in chunk_lines:\n",
    "        if line:\n",
    "            encoded_tokens.extend(tkn.encode(line))\n",
    "    return encoded_tokens\n",
    "\n",
    "def parallel_encode_file(input_path, output_path, chunk_size=10000):\n",
    "    \"\"\"\n",
    "    并行读取和编码一个大文件。\n",
    "    :param input_path: 输入的 txt 文件路径\n",
    "    :param output_path: 输出的 npy 文件路径\n",
    "    :param chunk_size: 每个并行任务处理的行数\n",
    "    \"\"\"\n",
    "    lines_chunk = []\n",
    "    all_encoded_tokens = []\n",
    "    \n",
    "    # 使用与BPE训练时相同的进程数，或者根据CPU核心数决定\n",
    "    num_processes = 8 # 或者 cpu_count()\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as f, Pool(processes=num_processes) as pool:\n",
    "        results = []\n",
    "        for line in f:\n",
    "            lines_chunk.append(line)\n",
    "            if len(lines_chunk) == chunk_size:\n",
    "                # 当达到块大小时，将任务提交给进程池\n",
    "                results.append(pool.apply_async(encode_chunk, (lines_chunk,)))\n",
    "                lines_chunk = []\n",
    "        \n",
    "        # 处理剩余的行\n",
    "        if lines_chunk:\n",
    "            results.append(pool.apply_async(encode_chunk, (lines_chunk,)))\n",
    "\n",
    "        # 收集所有并行任务的结果\n",
    "        for res in results:\n",
    "            all_encoded_tokens.extend(res.get())\n",
    "\n",
    "    # 转换为 NumPy 数组并保存\n",
    "    encodes = np.array(all_encoded_tokens, dtype=np.uint16)\n",
    "    np.save(output_path, encodes)\n",
    "    print(f\"Successfully encoded and saved to {output_path}\")\n",
    "\n",
    "\n",
    "# 对训练集和验证集进行并行编码\n",
    "for stage in ['train', 'valid']:\n",
    "    input_file = f'data/owt_{stage}.txt'\n",
    "    output_file = f'data/owt-{stage}.npy'\n",
    "    parallel_encode_file(input_file, output_file)\n",
    "\n",
    "# # encode the owt train/valid within block\n",
    "# for stage in ['train', 'valid']:\n",
    "#     encodes = []\n",
    "#     with open(f'data/owt_{stage}.txt', 'r', encoding = 'utf-8') as f:\n",
    "#         for line in f:\n",
    "#             if not line:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 encodes.extend(tkn.encode(line))\n",
    "#     encodes = np.array(encodes, dtype = np.uint16)\n",
    "#     np.save(f'data/owt-{stage}.npy', encodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
