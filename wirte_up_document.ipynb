{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2bd2dc3",
   "metadata": {},
   "source": [
    "#  Problem (unicode1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b1e48",
   "metadata": {},
   "source": [
    " (a) What Unicode character does chr(0) return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d14fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33949740",
   "metadata": {},
   "source": [
    " (b) How does this character’s string representation (`__repr__()`) differ from its printed representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "326712ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\u0000world\n"
     ]
    }
   ],
   "source": [
    "print('hello' + chr(0) + 'world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c5f7777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'hello\\\\x00world'\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr('hello' + chr(0) + 'world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd82df",
   "metadata": {},
   "source": [
    "打印以可读性为目标展示最终输出，会有不可见字符；`repr()`使用无歧义的转义字符以进行调试和理解等！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fc2e2",
   "metadata": {},
   "source": [
    "(c) What happens when this character occurs in text? It may be helpful to play around with the following in your Python interpreter and see if it matches your expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81def2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n",
      "hello\u0000world\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))\n",
    "print('hello' + chr(0) + 'world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0a4af7",
   "metadata": {},
   "source": [
    "#  Problem (unicode2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d5f7ab",
   "metadata": {},
   "source": [
    "**Unicode Encoding**:  it’s impractical to train tokenizers directly on Unicode codepoints, since the vocabulary would be prohibitively large (around 150K items) and sparse (since many characters are quite rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d1ec5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
      "<class 'bytes'>\n",
      "[104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33]\n",
      "List length: 23\n",
      "String length: 13\n",
      "hello! こんにちは!\n"
     ]
    }
   ],
   "source": [
    "# Example of UTF-8 encode\n",
    "test_string = 'hello! こんにちは!'\n",
    "utf8_encode = test_string.encode('utf-8')\n",
    "print(utf8_encode)\n",
    "print(type(utf8_encode)) # 'bytes': 8字节2进制数据 不可变\n",
    "utf8_encode_list = list(utf8_encode)\n",
    "print(utf8_encode_list)\n",
    "\n",
    "# UTF-8为可变长编码方法\n",
    "print(f'List length: {len(utf8_encode_list)}')\n",
    "print(f'String length: {len(test_string)}')\n",
    "\n",
    "print(utf8_encode.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f78dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xfeh\\x00e\\x00l\\x00l\\x00o\\x00!\\x00 \\x00S0\\x930k0a0o0!\\x00'\n",
      "<class 'bytes'>\n",
      "[255, 254, 104, 0, 101, 0, 108, 0, 108, 0, 111, 0, 33, 0, 32, 0, 83, 48, 147, 48, 107, 48, 97, 48, 111, 48, 33, 0]\n",
      "List length: 28\n",
      "String length: 13\n",
      "hello! こんにちは!\n"
     ]
    }
   ],
   "source": [
    "# Example of UTF-16 encode\n",
    "test_string = 'hello! こんにちは!'\n",
    "utf16_encode = test_string.encode('utf-16')\n",
    "print(utf16_encode)\n",
    "print(type(utf16_encode))\n",
    "utf16_encode_list = list(utf16_encode)\n",
    "print(utf16_encode_list)\n",
    "\n",
    "# UTF-16为可变长编码方法\n",
    "print(f'List length: {len(utf16_encode_list)}')\n",
    "print(f'String length: {len(test_string)}')\n",
    "\n",
    "print(utf16_encode.decode('utf-16'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c724343",
   "metadata": {},
   "source": [
    "(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings.\n",
    "\n",
    "1. UTF-8 空间最高效\n",
    "2. UTF-8 最为通用\n",
    "3. UTF-16/32 需要额外的BOM标记符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037bf87",
   "metadata": {},
   "source": [
    "(b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.\n",
    "\n",
    "(c) Give a two byte sequence that does not decode to any Unicode character(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d23cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "an incorrect example: 草\n"
     ]
    }
   ],
   "source": [
    "# 只对1byte字符有效\n",
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "print(decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\")))\n",
    "try:\n",
    "    print(decode_utf8_bytes_to_str_wrong(\"草\".encode(\"utf-8\")))\n",
    "except:\n",
    "    print(f'an incorrect example: 草')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b99c0dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-tokenizer for fast merge compute\n",
    "import regex as re\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b8d48de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31743/31743 [00:52<00:00, 608.17it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     56\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     57\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m             encodes.extend(\u001b[43mtkn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     59\u001b[39m encodes = np.array(encodes, dtype = np.uint16)\n\u001b[32m     60\u001b[39m np.save(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdata/owt_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.npy\u001b[39m\u001b[33m'\u001b[39m, encodes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cpfs01/projects-HDD/cfff-97b0176c46f5_HDD/zsq_24210980123/assignment1-basics/cs336_basics/Tokenizer/tokenizer.py:318\u001b[39m, in \u001b[36mTokenizer.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    316\u001b[39m             encode_result.append(\u001b[38;5;28mself\u001b[39m.inv_vocab[chunk_text.encode(\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)])\n\u001b[32m    317\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m             encode_result.extend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_text\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m encode_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cpfs01/projects-HDD/cfff-97b0176c46f5_HDD/zsq_24210980123/assignment1-basics/cs336_basics/Tokenizer/tokenizer.py:267\u001b[39m, in \u001b[36mTokenizer.chunk_encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m matching \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.extract_PAT.finditer(text):\n\u001b[32m    266\u001b[39m     pre_token = matching.group(\u001b[32m0\u001b[39m).encode(\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     pre_token = [\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m pre_token] \u001b[38;5;66;03m# List[bytes]\u001b[39;00m\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    270\u001b[39m         \u001b[38;5;66;03m# if all bytes are merged, end the loop\u001b[39;00m\n\u001b[32m    271\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pre_token) == \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# train and save (1) vocabulary (2) merges (3) encoding over openweb\n",
    "import base64\n",
    "import json\n",
    "from cs336_basics.Tokenizer.tokenizer import run_train_bpe, Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "# train vocab and merges\n",
    "vocab, merges = run_train_bpe(\n",
    "    'data/owt_valid.txt',\n",
    "    32000,\n",
    "    ['<|endoftext|>'],\n",
    "    **{'n_processes': 8}\n",
    ")\n",
    "\n",
    "# save vocab and merges\n",
    "# without loss encode\n",
    "vocab = {idx: base64.b64encode(token).decode('ascii')  for idx, token in vocab.items()}\n",
    "with open('data/owt_vocab.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(vocab, f, ensure_ascii=False, indent = 2)\n",
    "\n",
    "with open('data/owt_merges.txt', 'w', encoding = 'utf-8') as f:\n",
    "    for pair in merges:\n",
    "        first = base64.b64encode(pair[0]).decode('ascii')\n",
    "        second = base64.b64encode(pair[1]).decode('ascii')\n",
    "        f.write(f'{first} {second}\\n')\n",
    "\n",
    "\n",
    "# load vocab and merges\n",
    "with open('data/owt_vocab.json', 'r', encoding = 'utf-8') as f:\n",
    "    vocab = json.load(f)\n",
    "vocab = {idx: base64.b64decode(encode_b) for idx, encode_b in vocab.items()}\n",
    "\n",
    "merges = []\n",
    "with open('data/owt_merges.txt', 'r', encoding = 'utf-8') as f:\n",
    "    for row in f:\n",
    "        if not row:\n",
    "            continue\n",
    "        parts = row.split()\n",
    "        if len(parts) == 2:\n",
    "            first = base64.b64decode(parts[0])\n",
    "            second = base64.b64decode(parts[1])\n",
    "            merges.append((first, second))\n",
    "\n",
    "tkn = Tokenizer(\n",
    "    vocab, merges, ['<|endoftext|>']\n",
    ")\n",
    "\n",
    "# encode the owt train/valid within block\n",
    "for stage in ['train', 'valid']:\n",
    "    encodes = []\n",
    "    with open(f'data/owt_{stage}.txt', 'r', encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            if not line:\n",
    "                continue\n",
    "            else:\n",
    "                encodes.extend(tkn.encode(line))\n",
    "    encodes = np.array(encodes, dtype = np.uint16)\n",
    "    np.save(f'data/owt_{stage}.npy', encodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c1897cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31743/31743 [00:03<00:00, 9167.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b' stillness'\n",
      "[(b' ', b't'), (b'h', b'e'), (b' ', b'a'), (b' ', b's'), (b' ', b'w'), (b'n', b'd'), (b' t', b'he'), (b'e', b'd'), (b' ', b'b'), (b' t', b'o')]\n"
     ]
    }
   ],
   "source": [
    "# Just test my implementation (fast enough)\n",
    "from cs336_basics.Tokenizer.tokenizer import run_train_bpe\n",
    "\n",
    "vocab, merges = run_train_bpe(\n",
    "    'data/TinyStoriesV2-GPT4-train.txt',\n",
    "    32000,\n",
    "    ['<|endoftext|>'],\n",
    "    **{'n_processes': 8}\n",
    ")\n",
    "\n",
    "print(vocab[20000])\n",
    "print(merges[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
