{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2bd2dc3",
   "metadata": {},
   "source": [
    "#  Problem (unicode1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42b1e48",
   "metadata": {},
   "source": [
    " (a) What Unicode character does chr(0) return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d14fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33949740",
   "metadata": {},
   "source": [
    " (b) How does this character’s string representation (`__repr__()`) differ from its printed representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326712ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello' + chr(0) + 'world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f7777",
   "metadata": {},
   "outputs": [],
   "source": [
    "repr('hello' + chr(0) + 'world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd82df",
   "metadata": {},
   "source": [
    "打印以可读性为目标展示最终输出，会有不可见字符；`repr()`使用无歧义的转义字符以进行调试和理解等！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63fc2e2",
   "metadata": {},
   "source": [
    "(c) What happens when this character occurs in text? It may be helpful to play around with the following in your Python interpreter and see if it matches your expectations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81def2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chr(0))\n",
    "print('hello' + chr(0) + 'world')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0a4af7",
   "metadata": {},
   "source": [
    "#  Problem (unicode2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d5f7ab",
   "metadata": {},
   "source": [
    "**Unicode Encoding**:  it’s impractical to train tokenizers directly on Unicode codepoints, since the vocabulary would be prohibitively large (around 150K items) and sparse (since many characters are quite rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ec5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of UTF-8 encode\n",
    "test_string = 'hello! こんにちは!'\n",
    "utf8_encode = test_string.encode('utf-8')\n",
    "print(utf8_encode)\n",
    "print(type(utf8_encode)) # 'bytes': 8字节2进制数据 不可变\n",
    "utf8_encode_list = list(utf8_encode)\n",
    "print(utf8_encode_list)\n",
    "\n",
    "# UTF-8为可变长编码方法\n",
    "print(f'List length: {len(utf8_encode_list)}')\n",
    "print(f'String length: {len(test_string)}')\n",
    "\n",
    "print(utf8_encode.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f78dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of UTF-16 encode\n",
    "test_string = 'hello! こんにちは!'\n",
    "utf16_encode = test_string.encode('utf-16')\n",
    "print(utf16_encode)\n",
    "print(type(utf16_encode))\n",
    "utf16_encode_list = list(utf16_encode)\n",
    "print(utf16_encode_list)\n",
    "\n",
    "# UTF-16为可变长编码方法\n",
    "print(f'List length: {len(utf16_encode_list)}')\n",
    "print(f'String length: {len(test_string)}')\n",
    "\n",
    "print(utf16_encode.decode('utf-16'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c724343",
   "metadata": {},
   "source": [
    "(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various input strings.\n",
    "\n",
    "1. UTF-8 空间最高效\n",
    "2. UTF-8 最为通用\n",
    "3. UTF-16/32 需要额外的BOM标记符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5037bf87",
   "metadata": {},
   "source": [
    "(b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results.\n",
    "\n",
    "(c) Give a two byte sequence that does not decode to any Unicode character(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只对1byte字符有效\n",
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "print(decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\")))\n",
    "try:\n",
    "    print(decode_utf8_bytes_to_str_wrong(\"草\".encode(\"utf-8\")))\n",
    "except:\n",
    "    print(f'an incorrect example: 草')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c0dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-tokenizer for fast merge compute\n",
    "import regex as re\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d48de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9743/9743 [00:02<00:00, 4343.43it/s] \n"
     ]
    }
   ],
   "source": [
    "# train and save (1) vocabulary (2) merges (3) encoding over openweb\n",
    "import base64\n",
    "import json\n",
    "from cs336_basics.Tokenizer.tokenizer import run_train_bpe, Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "# train vocab and merges\n",
    "vocab, merges = run_train_bpe(\n",
    "    'data/TinyStoriesV2-GPT4-train.txt',\n",
    "    10000,\n",
    "    ['<|endoftext|>'],\n",
    "    **{'n_processes': 8}\n",
    ")\n",
    "\n",
    "# save vocab and merges\n",
    "# without loss encode\n",
    "vocab = {idx: base64.b64encode(token).decode('ascii')  for idx, token in vocab.items()}\n",
    "with open('data/TinyStoriesV2-GPT4-vocab.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(vocab, f, ensure_ascii=False, indent = 2)\n",
    "\n",
    "with open('data/TinyStoriesV2-GPT4-merges.txt', 'w', encoding = 'utf-8') as f:\n",
    "    for pair in merges:\n",
    "        first = base64.b64encode(pair[0]).decode('ascii')\n",
    "        second = base64.b64encode(pair[1]).decode('ascii')\n",
    "        f.write(f'{first} {second}\\n')\n",
    "\n",
    "\n",
    "# load vocab and merges\n",
    "with open('data/TinyStoriesV2-GPT4-vocab.json', 'r', encoding = 'utf-8') as f:\n",
    "    vocab = json.load(f)\n",
    "vocab = {int(idx): base64.b64decode(encode_b) for idx, encode_b in vocab.items()}\n",
    "\n",
    "merges = []\n",
    "with open('data/TinyStoriesV2-GPT4-merges.txt', 'r', encoding = 'utf-8') as f:\n",
    "    for row in f:\n",
    "        if not row:\n",
    "            continue\n",
    "        parts = row.split()\n",
    "        if len(parts) == 2:\n",
    "            first = base64.b64decode(parts[0])\n",
    "            second = base64.b64decode(parts[1])\n",
    "            merges.append((first, second))\n",
    "\n",
    "tkn = Tokenizer(\n",
    "    vocab, merges, ['<|endoftext|>']\n",
    ")\n",
    "\n",
    "# encode the owt train/valid within block\n",
    "for stage in ['train', 'valid']:\n",
    "    encodes = []\n",
    "    with open(f'data/TinyStoriesV2-GPT4-{stage}.txt', 'r', encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            if not line:\n",
    "                continue\n",
    "            else:\n",
    "                encodes.extend(tkn.encode(line))\n",
    "    encodes = np.array(encodes, dtype = np.uint16)\n",
    "    np.save(f'data/TinyStoriesV2-GPT4-{stage}.npy', encodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1897cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just test my implementation (fast enough)\n",
    "from cs336_basics.Tokenizer.tokenizer import run_train_bpe\n",
    "\n",
    "vocab, merges = run_train_bpe(\n",
    "    'data/TinyStoriesV2-GPT4-train.txt',\n",
    "    32000,\n",
    "    ['<|endoftext|>'],\n",
    "    **{'n_processes': 8}\n",
    ")\n",
    "\n",
    "print(vocab[20000])\n",
    "print(merges[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
